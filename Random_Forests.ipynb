{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Forests.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYRF4CwdRche"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAjp8Dh9RlIa"
      },
      "source": [
        "## Random Forests\n",
        "\n",
        "In previous lessons we've covered the basics of decision trees and bootstrap aggregating, but in this lesson, we'll turn our spotlight solely on Random Forests.\n",
        "\n",
        "### *Why Random Forests?*\n",
        "\n",
        "  A random forest uses the age old saying \"two (or more) heads are better\" than one and applies it to decsion trees. We saw that a decision tree splits the dataset on various features and that it's essential to place priority on feature splits that best seperate the dataset.\n",
        "\n",
        "Even though they are widely applicable and easy to visualize, *decision trees have their downsides*:\n",
        "- At *high depths*, they can create unnecessarily complex trees that *overfit* the data.\n",
        "- Can be very highly unpredictable when slightly changing up the dataset.\n",
        "- Since decision trees rely on *heuristics* (basically a user-defined metric to say how well the tree is splitting up the dataset) those heuristics can't be guaranteed to always find the optimum tree.\n",
        "\n",
        "These above problems that a single decision tree faces can be mitigated by a little help from its friends... more decision trees!\n",
        "\n",
        "Essentially a random forest divys up the tasks of finding the best features to split on among different decision trees and let's them run their course on these features. After some defined time to stop, the trees congregate together in an *ensemble* to compare their findings and find an average feature split that works best off of what they've found.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdCLqqflWcP7"
      },
      "source": [
        "# Question 1. Let's Explore This Forest! But, First, A Little Recap\n",
        "\n",
        "I know, I want to get right to creating Random Forests too, but we need to take a step back and make sure we understand how decision trees work.\n",
        "\n",
        "Fill in the equations for Gini Impurity and Entropy and run the following cell to see them graphed for the range of proabilites $ 0 \\leq p_i \\leq 1 $.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmgB4tZOP-lv"
      },
      "source": [
        "import math\n",
        "## -- TO DO -- ##\n",
        "def gini_impurity(probability):\n",
        "  return \n",
        "## -- END -- #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1hVHMJ2z_rU"
      },
      "source": [
        "## -- TO DO -- ##\n",
        "def entropy(probability):\n",
        "  return \n",
        "## -- END -- ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW8Xz_tMQT9b"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "vals = np.arange(.05, 1, .01)\n",
        "gini_impurity_vals = [gini_impurity(prob) for prob in vals] #To fill in\n",
        "entropy_vals = [entropy(prob) for prob in vals] #To fill in \n",
        "plt.plot(vals, gini_impurity_vals, label=\"Gini Impurity\")\n",
        "plt.plot(vals, entropy_vals, label=\"Entropy\")\n",
        "plt.xlabel(\"Probability\")\n",
        "plt.ylabel(\"Function Values\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD5YHh9r7Zee"
      },
      "source": [
        "# Question 2. Refining Our Data\n",
        "Now let's get working towards predicting these MPGs for Jones from the notes. We'll be working with a dataset originally from the Statlib library at Carnegie Mellon.\n",
        "\n",
        "### Dataset Properties\n",
        "1.  1 **mpg** column for the car's fuel efficiency in miles per gallon.\n",
        "2. 8 columns of other car attributes: 5 continuous and 3 categorical.\n",
        "3. 398 rows/instances of cars.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7gybxg9WyNC"
      },
      "source": [
        "## Q 2.1: Importing the Dataset and Initial Impressions\n",
        "Let's first import this dataset and take a peek at the columns and their values to\n",
        " see how we can divide our MPG values into classes to get the ball (or tire) rolling!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T5qHqoYWzIl"
      },
      "source": [
        "#import the auto-mpg csv in using pandas\n",
        "cars = pd.read_csv(\"auto-mpg.csv\")\n",
        "cars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9spcBJFxy_M7"
      },
      "source": [
        "## -- TO DO -- ##\n",
        "## Plot the mpg values using pandas as a histogram and fill in the max and min mpgs ##\n",
        "\n",
        "mpg_max = ...\n",
        "mpg_min = ...\n",
        "## -- END -- ##\n",
        "print(\"Highest (Most Efficient) MPG: \", mpg_max) ## SHOULD EQUAL 46.6\n",
        "print(\"Lowest (Least Efficient) MPG: \", mpg_min) ## SHOULD EQUAL 9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O043aeuCaN2f"
      },
      "source": [
        "### Q2.1.1 What did you find as the largest and smallest miles per gallons?\n",
        "```\n",
        "# Write your answer here:\n",
        "#\n",
        "```\n",
        "\n",
        "As we can see, there's quite a range in MPG that a car could have. They are also skewed to one side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYc1WpLkXJrW"
      },
      "source": [
        "## Q 2.2: Adjusting Our Data\r\n",
        "\r\n",
        "To make our process easier, lets divide our data up into four named categories, depending on the range of MPGs it falls under and how much we'd want to buy it.\r\n",
        "- **0:**  (5.0 < MPG <= 14.9)\r\n",
        "- **1:**  (15.0 < MPG <= 24.9)\r\n",
        "- **2:**  (25.0 < MPG <= 34.9)\r\n",
        "- **3:**  ( 35.0 < MPG <= 50.0)\r\n",
        "\r\n",
        "\r\n",
        "Let's now create a **desire_creator** function, which outputs an array created from the MPG column of the **cars** dataset, where if the MPG falls into a certain value range as specified above, we insert the corresponding value.\r\n",
        "\r\n",
        "For example, if the mpgs column were an array:\r\n",
        "```\r\n",
        "mpgs = [34.5, 42.3, 22.9]\r\n",
        "desires = desire_creator(mpgs)\r\n",
        "print(desires)\r\n",
        "[2, 3, 1]\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNNiwBp1X2eg"
      },
      "source": [
        "# Should return an array containing the desirability ranking deduced from the mpg \n",
        "# cars_dataset: \n",
        "def desire_creator(cars_dataset):\n",
        "  desires = []\n",
        "  ## -- TO DO -- ##\n",
        "  ## -- END -- ##\n",
        "  return np.asarray(desires)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP4mdGBoT4Cm"
      },
      "source": [
        "desirability = desire_creator(cars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_T24lsXid5O"
      },
      "source": [
        "Let's create a new dataset named **cars_and_desires** replacing the **mpg** column with **desirability**.\n",
        "\n",
        "Also, since sklearn's random tree classifier only works numeric data, we'll have to drop the **car name** and **horspepower** columns, given they are *string* and *object* types respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unuBC9ldkHWW"
      },
      "source": [
        "## -- TO DO: -- ##\n",
        "cars_and_desires = ...\n",
        "## -- END -- ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEM1p6CCk1jw"
      },
      "source": [
        "cars_and_desires"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHttVCEvthYI"
      },
      "source": [
        "Let's see this histogram of the desirabilities to get an idea of what proportion of cars are desirable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G26ffy9atr_d"
      },
      "source": [
        "cars_and_desires.desirability.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AsqLygfQN44"
      },
      "source": [
        "Now that we have our data/attributes and labels together... it only makes sense to split them apart, right?\n",
        "\n",
        "Let's make our data `X` and our features `y` given an `index`, where to the left of the index is the test data, to the right, the labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m2iMaz_Qkyl"
      },
      "source": [
        "# Splits the dataset into an X of the data/attributes, and y of the features on a given index.\n",
        "# dataset: incoming dataset to be split\n",
        "# index: the index of final column that is be split off to create the features\n",
        "def data_label_splitter(dataset, index):\n",
        "  ## -- TO DO -- ##\n",
        "  X = = ...\n",
        "  y = ...\n",
        "  ## -- END -- ##\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7PDeForSkqs"
      },
      "source": [
        "Let's now fill in this function to divide our `X` and `y` into test trains splits given a `test_fraction`.\n",
        "\n",
        "##### *Hint:* Look at the imports at the top of the page.\n",
        "##### *Make sure to set `random_state = 42` otherwise, we may run into problems down the line.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTA_8ASfSrSZ"
      },
      "source": [
        "# Splits the dataset into testing and training data:\n",
        "# X: The data matrix\n",
        "# y: The labels column\n",
        "# test_fraction: What fraction (float) of the data will go to the test set\n",
        "def train_test_splitter(X, y, test_fraction):\n",
        "  ##-- TO DO --##\n",
        "  return \n",
        "  ## -- END -- ##\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-YKUdzWYgAo"
      },
      "source": [
        "I know filling out these functions is annoying -- they'll come in handy later, I swear. Fill in the method parameters below. Let's first have our testing data to be $\\frac{2}{10}$ of the overall dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVFTAibSWagR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "test_fraction = ## Assign fraction value\n",
        "X, y = data_label_splitter() ## Fill in method parameter\n",
        "X_train, X_test, y_train, y_test = train_test_splitter() ## Fill in method parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "285Fhnxq6kPt"
      },
      "source": [
        "We've now got our car data all prepared for Jones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjaqTZbnN5u5"
      },
      "source": [
        "# Question 3: Classification\n",
        "Now that we finally have our test and training data set up, all we have to do is create a Decision Tree Classifier, fit our X and y training data to the classifier, and see how the prediction on training data performs.\n",
        "\n",
        "We'll use Scikit-Learn (`sklearn`) to first create a Decision Tree Classifier and run some tests to see how accurately Jones' could perform doing the searching himself, without an ensemble of henchmen to aggregate their findings.\n",
        "\n",
        "Then, we'll then use Scikit-Learn's Random Forest Classifier to simulate if Jones did employ his henchmen. After we do this, we'll compare the results of the Decision Tree and Random Forest to report back to Jones if employing his henchman is a good idea or not.\n",
        "\n",
        "##### *Make sure to set `random_state = 42` otherwise, we may run into problems down the line.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEQ_NAnWIrkr"
      },
      "source": [
        "## Q 3.1: Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIKe7qAH-xuK"
      },
      "source": [
        "from sklearn.tree import  DecisionTreeClassifier\n",
        "from sklearn.metrics import  accuracy_score\n",
        "\n",
        "tree_classifier = tree.DecisionTreeClassifier() ## To Fill In\n",
        "tree_classifier.fit() # To Fill in \n",
        "tree_y_pred = tree_classifier.predict() #To Fill in \n",
        "\n",
        "accuracy = accuracy_score() # To Fill in\n",
        "print(accuracy)\n",
        "\n",
        "assert(.73 <= accuracy and accuracy <= .74)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwrQHZGs9F8O"
      },
      "source": [
        "Try running the above cell a few times without a `random_state` set and see if the accuracy changes. *Did you fix `random_state` to a certain value?* Make sure you understand what assigning `random_state` does within Scikit functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mLNoRKQ5Ug6"
      },
      "source": [
        "## Q 3.2 Random Forests Classifier\r\n",
        "Let's now create our Random Forest Classifier, with `20` trees (aka 20 of Jones' henchmen)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub_-gwl6AGdX"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_classifier = ... ## To Fill In\n",
        "accuracy = ... \n",
        " # To Fill In\n",
        "print(accuracy)\n",
        "\n",
        "assert(.72 <= accuracy and accuracy <= .79)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ee0ekeo59iJ"
      },
      "source": [
        "#### Q 3.2.1 *Does the forest perform better than a tree? Is it worth it for Jones to employ `20` henchmen?*\n",
        "\n",
        "\n",
        "```\n",
        "# Write you answer here:\n",
        "# \n",
        "```\n",
        "\n",
        "*How about `50`, `100`, and `500`?* Run the cell above on each a few times with each count. Note how the variance in accuracies change.\n",
        "\n",
        "Below fill in the function which returns the accuracy of a single run of the random forest classifier.\n",
        "\n",
        "`splits` should be a list of three variables, with each index consisting of:\n",
        "- **index 0:** training data\n",
        "- **index 1:** training labels\n",
        "- **index 2:** test data\n",
        "- **index 3:** test labels\n",
        "\n",
        "In the second block of code, experiment with differing `number_of_trees` and `num_rand_seeds` values. \n",
        "#### Q 3.2.2 *Do you notice any trends related to the number of trees and the metrics of accuracies?*\n",
        "\n",
        "```\n",
        "# Write your answer here:\n",
        "# \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T22TgotbanKo"
      },
      "source": [
        "Fill in the function `random_forest_test` below which creates a `RandomForestClassifier` given parameters number of trees (`num_trees`), data and features which have been test-train split into the values of (`splits`), and a random seed (`rand_seed`).\r\n",
        "\r\n",
        "`splits` should be in the format:\r\n",
        "- `splits[0]`: Traning data\r\n",
        "- `splits[1]`: Traning labels\r\n",
        "- `splits[2]`: Test data\r\n",
        "- `splits[3]`: Test labels\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldT9AhdW_-fl"
      },
      "source": [
        "num_rand_seeds = ... # Experiment with this value\n",
        "random_seeds = np.random.randint(100, size=num_rand_seeds)\n",
        "splits = ... # To Fill In\n",
        "def random_forest_test(num_trees, splits, rand_seed):\n",
        "  ## -- TO DO -- ##\n",
        "\n",
        "  ## -- END -- ##\n",
        "  return accuracy\n",
        "\n",
        "## The code below prints the average accuracy from 5 runs of random_forest_test(), with each of the random seeds set above ##\n",
        "number_of_trees = ... ## Experiment with this value.\n",
        "assert(1 <= number_of_trees and number_of_trees <= 500)\n",
        "accuracies = []\n",
        "for seed in random_seeds:\n",
        "  accs = []\n",
        "  for i in np.arange(5):\n",
        "    accs.append(random_forest_test(number_of_trees, splits, seed))\n",
        "  accuracies.append(np.mean(accs))\n",
        "\n",
        "\n",
        "print(accuracies)\n",
        "print(\"Min. Acc. :\", min(accuracies))\n",
        "print(\"Max. Acc. :\", max(accuracies))\n",
        "print(\"Avg. Acc. :\", np.mean(accuracies))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGsT6HGBbUWn"
      },
      "source": [
        "Increasing the number of trees will come closer to reaching a uniform solution regardless of randomness. Even if Jones selects different henchmen who are bound to go about choosing the desired features in different ways, having a large enough of number of trees/henchmen in the ensemble will reach a standardized solution *given the same training and test data.*\n",
        "\n",
        "Jones wants to know the ideal number of henchman to employ so he doesn't have any of them wasting their time. Let's plot how the # of trees/henchmen affect the accuracy.\n",
        "\n",
        "#### Q 3.2.3 *Do you think there will be a dropoff, why or why not?*\n",
        "\n",
        "```\n",
        "# Write your answer below:\n",
        "#\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6notKvsFcTh"
      },
      "source": [
        "tree_counts = np.arange(1, 500, 20)\n",
        "tree_labels = tree_counts.astype(str)\n",
        "accs_on_tree_counts = []\n",
        "for count in tree_counts:\n",
        "  rand_seeds = np.random.randint(100, size=1)\n",
        "  accs = []\n",
        "  for seed in rand_seeds:\n",
        "    accs.append(random_forest_test(count, splits, seed))\n",
        "  accs_on_tree_counts.append(np.mean(accs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGI86Hpjo7sH"
      },
      "source": [
        "plt.plot(tree_counts, accs_on_tree_counts, label=\"Accuracy\")\n",
        "plt.xlabel(\"Number of Trees in Random Forest\")\n",
        "plt.ylabel(\"Accuracy Percentage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp7ULoE6sgML"
      },
      "source": [
        "#### Q 3.2.4 *Around what accuracy does the classifer reach before jumping around randomly?*\n",
        "\n",
        "```\n",
        "# Write your answer below:\n",
        "# \n",
        "```\n",
        "#### Q 3.2.5 *Intuitively, why does the accuracy rating appear to jump around between a few set values? What is going on here?* **Hint:** *Is this a large dataset? / Think about swing states/counties in elections.*\n",
        "```\n",
        "# Write your answer below: \n",
        "# \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXVOL2pnUqkO"
      },
      "source": [
        "Let's quickly try scaling our features of our data and see if that affects the accuracy of our Random Forests classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQSqdmztVfbu"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\r\n",
        "scaler = StandardScaler()\r\n",
        "X_train_scaled = scaler.fit_transform(X_train)\r\n",
        "forest_classifier = RandomForestClassifier(n_estimators=50, random_state=42, criterion=\"gini\")\r\n",
        "forest_classifier.fit(X_train_scaled, y_train)\r\n",
        "X_test_scaled = scaler.transform(X_test)\r\n",
        "forest_y_pred_scaled = forest_classifier.predict(X_test_scaled)\r\n",
        "\r\n",
        "scaled_accuracy = accuracy_score(y_test, forest_y_pred_scaled)\r\n",
        "print(scaled_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcSB1qLMZKYW"
      },
      "source": [
        "#### Q *3.2.6 Did scaling the data affect the accuracy of the Random Forests Classifier? Why is this so?*\r\n",
        "```\r\n",
        "# Write your answer here:\r\n",
        "# \r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxeAG7N3uLql"
      },
      "source": [
        "## Q4 Visualization\n",
        "\n",
        "Now Jones is curious to know exactly how these decision trees work. He wants us now to visualize a few decision trees to see how the features are being split. For this we'll import `graphviz` and the addon library `dtreeviz`\n",
        "\n",
        "More info on `dtreeviz` can be found on its github page: https://github.com/parrt/dtreeviz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rewf-6Hvu_ef"
      },
      "source": [
        "# Importing graphviz and installing dtreeviz\n",
        "import graphviz\n",
        "!pip install dtreeviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF0EFYQOk5jk"
      },
      "source": [
        "Part of this data analytics job is presenting the information back to the client in a digestible and engaging fashion. \r\n",
        "Assign each of the 4 categories of MPGs an adjective you see fit in `target_names`. For example, `target_names[0] = \"horrible\"` or `target_names[3] = \"ideal\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIftD9w0vD1X"
      },
      "source": [
        "target_names = ... # Assign four labels for the four groups\n",
        "dot_data = tree.export_graphviz(tree_classifier, out_file=None, feature_names=X_train.columns, class_names=target_names, filled=True) ## To Fill In\n",
        "graph = graphviz.Source(dot_data, format=\"png\")\n",
        "graph\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMsNxp7syl0N"
      },
      "source": [
        "### Q 4.1 Interpreting the Visual Data\n",
        "\n",
        "Browse around the tree. Do you see clusters of Nodes with various ratings? Look a little upstream and report some desicions/splits that help find the most desirable cars, and some that find the least desirable cars. \n",
        "\n",
        "#### Q 4.1.1 *How do you interpret the features and numbers contained in these splits? Do the most substantial splits performed at a low depth practically make sense to you in choosing a car which has a high MPG?*\n",
        "\n",
        "```\n",
        "# Write your answers below:\n",
        "#\n",
        "```\n",
        "\n",
        "#### Q 4.1.2 *Do you notice any positive or negative outliers among groups of nodes at a high depth in the decision tree?* *What are some splits that precede these outliers?*\n",
        "\n",
        "**Positive:**\n",
        "```\n",
        "# Write your answer(s) below: \n",
        "# \n",
        "```\n",
        "**Negative:**\n",
        "```\n",
        "# Write your answer(s) below:\n",
        "# \n",
        "```\n",
        "\n",
        "#### Q 4.1.3 *What is value of Gini Impurity when leaf nodes are created? Why is that so? What is occuring when the Gini Impurity is 0.5?*\n",
        "\n",
        "```\n",
        "# Write your answers below:\n",
        "# \n",
        "```\n",
        "#### Q 4.1.4 Write down the three most crucial splits (in order) you would report back to Jones, to most easily find the best rated cars.\n",
        "```\n",
        "# Write your answers below: \n",
        "# \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0bQCCw5d16a"
      },
      "source": [
        "Jones also wants to know exactly how important each feature is to Random Forests classification. Luckily sklearn's `RandomForestClassifer` has a helpful parameter `feature_importances_` for this task.\r\n",
        "\r\n",
        "Using `forest_classifer` from Question 3.2 and matplotlib, make a bar plot of each feature and its importance score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Iuzp6rNfqcC"
      },
      "source": [
        "## -- TO DO -- ##\r\n",
        "\r\n",
        "## -- END -- ##\r\n",
        "# The plot should have sensible labels, title, scale, and information"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLbNNEinh000"
      },
      "source": [
        "#### Q 4.1.5 *What are the three most important features?*\r\n",
        "```\r\n",
        "# Write your answers below:\r\n",
        "# \r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmiyoSWLuUDa"
      },
      "source": [
        "That wraps it up! Jones thanks you deeply for your help."
      ]
    }
  ]
}